{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Akshay Pozath\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Twitter Data Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_FL--h5apPw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOOqNiaJapPy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSnwuTGlapPy"
   },
   "source": [
    "## Accessing Twitter API  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Keys.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "s3GABOPEapP0",
    "outputId": "22edcb96-5f48-4682-8d8c-ed669bca9294"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/akshaypozath/Library/Messages/Attachments/ea/10/6D0E76BA-2EA3-4C33-A35A-069EE9C29EFF'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "key_file = 'keys.json'\n",
    "\n",
    "with open(key_file) as f:\n",
    "    keys = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeF48lesapP1",
    "outputId": "7d96ba32-cb41-4356-b10d-b0f57480bb96"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import TweepyException\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    auth = tweepy.OAuthHandler(keys[\"consumer_key\"], keys[\"consumer_secret\"])\n",
    "    redirect_url = auth.get_authorization_url()\n",
    "    auth.set_access_token(keys[\"access_token\"], keys[\"access_token_secret\"])\n",
    "    api = tweepy.API(auth)\n",
    "    print(\"Rutgers username is:\", api.get_user(screen_name=\"RutgersU\").name)\n",
    "except TweepyException as e:\n",
    "    logging.warning(\"There was a Tweepy error. Double check your API keys and try again.\")\n",
    "    logging.warning(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Refactor and Extend Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keys(path):\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_recent_tweets_by_user(user_account_name, keys):\n",
    "   \n",
    "    import tweepy\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets(path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_with_cache(user_account_name, keys_path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "rutgers_tweets = download_recent_tweets_by_user(\"RutgersU\", key_file)\n",
    "print(\"Number of tweets downloaded:\", len(rutgers_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usVPef0dapP2"
   },
   "source": [
    "## Working with Twitter Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtJWj1sVapP2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "ds_tweets_save_path = \"/home/ap1877/shared/RutgersU_recent_tweets.json\"   \n",
    "\n",
    "\n",
    "if not Path(ds_tweets_save_path).is_file():\n",
    "    \n",
    "    example_tweets = [t._json for t in tweepy.Cursor(api.user_timeline, screen_name=\"RutgersU\", \n",
    "                                             tweet_mode='extended').items()]\n",
    "    \n",
    "    \n",
    "    with open(ds_tweets_save_path, \"w\") as f:        \n",
    "        json.dump(example_tweets, f)\n",
    "\n",
    "\n",
    "with open(ds_tweets_save_path, \"r\") as f:\n",
    "    example_tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLq9NDhAapP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_tweets(path):\n",
    "    \"\"\"Loads tweets that have previously been saved.\n",
    "    \n",
    "    Calling load_tweets(path) after save_tweets(tweets, path)\n",
    "    will produce the same list of tweets.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The place where the tweets will be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictionary objects, each representing one tweet.\"\"\"\n",
    "    \n",
    "    with open(path, \"rb\") as f:\n",
    "        import json\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xW1xN_3XapP6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dest_path = \"/home/ap1877/shared/TrumpTweets_1.json\"\n",
    "trump_tweets = load_tweets(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i_aHCjSapP6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i, tweet in enumerate(trump_tweets[:10], 1):\n",
    "    print(f'Tweet {i}:')\n",
    "    print(f'Text: {tweet.get(\"full_text\", \"N/A\")}')\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjYXBPCpapP6"
   },
   "source": [
    "### Oldest Tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYPMcwPdapP6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "trump_tweets = pd.DataFrame(trump_tweets)\n",
    "\n",
    "\n",
    "def oldest_tweet(df):\n",
    "    month = df['created_at'].str[4:7]\n",
    "    month = month.rename('month')\n",
    "    year = df['created_at'].str[-4:]\n",
    "    year = year.rename('year')\n",
    "\n",
    "    oldest_month = pd.concat([month,year], axis=1)\n",
    "    minyear = oldest_month['year'].min()\n",
    "    oldest_month = oldest_month[oldest_month['year']=='2017']\n",
    "    m = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, \n",
    "     'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "\n",
    "    oldest_month['month'] = oldest_month['month'].map(m)\n",
    "    minmonth = oldest_month['month'].min()\n",
    "\n",
    "    return (minmonth)\n",
    "\n",
    "\n",
    "oldest_tweet_month = oldest_tweet(trump_tweets)\n",
    "print(oldest_tweet_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNdIIfM4apP6"
   },
   "source": [
    "## Twitter Source Analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnZTwTGRapP6"
   },
   "source": [
    "### Merge the dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKw0vaWTapP6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "path2 =  \"/home/ap1877/shared/TrumpTweets_2.json\"\n",
    "trump_tweets2 = load_tweets(path2)\n",
    "trump_tweets2 = pd.DataFrame(trump_tweets2)\n",
    "\n",
    "path3 =  \"/home/ap1877/shared/TrumpTweets_3.json\"\n",
    "trump_tweets3 = load_tweets(path3)\n",
    "trump_tweets3 = pd.DataFrame(trump_tweets3)\n",
    "\n",
    "all_tweets = pd.concat([trump_tweets, trump_tweets2, trump_tweets3]).drop_duplicates(subset='id', keep='first').reset_index(drop=True)\n",
    "### END ANSWER\n",
    "\n",
    "all_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2f3pX1TapP6"
   },
   "source": [
    "### Constructing DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaBrQPLNapP6"
   },
   "outputs": [],
   "source": [
    "\n",
    "all_tweets = all_tweets.astype({'full_text':'str'})\n",
    "text = []\n",
    "date = []\n",
    "\n",
    "\n",
    "for index, row in all_tweets.iterrows():\n",
    "    if row['full_text']=='nan':\n",
    "        text.append(row['full_text'])\n",
    "    else:\n",
    "        text.append(row['full_text'])\n",
    "    date.append(pd.to_datetime(row['created_at']))\n",
    "df_trump = all_tweets[['id', 'source','retweet_count','favorite_count']]\n",
    "df_trump.insert(1, \"time\", date, True)\n",
    "df_trump.insert(3, \"text\", text, True)\n",
    "df_trump = df_trump.astype({'id':'str'})\n",
    "df_trump = df_trump.sort_values(by=['id'])\n",
    "df_trump = df_trump.set_index('id')\n",
    "df_trump = df_trump.rename(columns={\"created_at\": \"time\"})\n",
    "df_trump = df_trump[~df_trump.index.duplicated(keep='first')]\n",
    "df_trump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiXXHDmpapP7"
   },
   "outputs": [],
   "source": [
    "df_trump['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uys4JaIuapP7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "   \n",
    "df_trump['source'] = df_trump['source'].str.replace(r'<[^>]*>','')\n",
    "df_trump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZo4QU7MapP7"
   },
   "source": [
    "####  Most Common Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOdz32WZapP7"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "device_list = df_trump['source']\n",
    "device_list = device_list.groupby(device_list)\n",
    "device_list = device_list.count()\n",
    "device_list\n",
    "keyList = device_list.keys().to_list()\n",
    "valList = device_list.to_list()\n",
    "\n",
    "df_v2 = pd.DataFrame(valList, columns=['counts'], index=keyList)\n",
    "df_v2 = df_v2.sort_values(by='counts', ascending=True)\n",
    "ax = df_v2.plot.barh(legend=False)\n",
    "ax.set_xlabel('Number of Tweets')\n",
    "print(df_v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZatUeX2capP7"
   },
   "source": [
    "### Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhBLwCxcapP7"
   },
   "outputs": [],
   "source": [
    "df_trump['time'][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uB-3WK4apP7"
   },
   "outputs": [],
   "source": [
    "df_trump['est_time'] = (\n",
    "    df_trump['time'] # Set initial timezone to UTC\n",
    "                 .dt.tz_convert(\"EST\") # Convert to Eastern Time\n",
    ")\n",
    "df_trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhkJKBxeapP8"
   },
   "outputs": [],
   "source": [
    "df_trump['hour'] =  df_trump['time'].dt.hour + df_trump['time'].dt.minute/60 + df_trump['time'].dt.second/3600\n",
    "df_trump['roundhour']=round(df_trump['hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQkwODiAapP8"
   },
   "outputs": [],
   "source": [
    "\n",
    "thehour = df_trump['roundhour'].groupby(df_trump['roundhour'])\n",
    "thehour = thehour.count()\n",
    "thehour[24] = thehour[24]+thehour[0]\n",
    "thehour = thehour.drop(0)\n",
    "thehour\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "sns.barplot(x=thehour.index, y=thehour.values, orient='v')\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.set_title('Times that Trump uses Twitter')\n",
    "ax.set_ylabel('Count of Tweets')\n",
    "ax.set_xlabel('Hour of day')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lXwAyKFapP8"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "top2devices = devices.nlargest(2).keys()\n",
    "dist1 = df_trump[df_trump['source'] == top2devices[0]]\n",
    "dist2 = df_trump[df_trump['source'] == top2devices[1]]\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "sns.distplot(dist1['hour'], hist=False, kde=True)\n",
    "sns.distplot(dist2['hour'], hist=False, kde=True)\n",
    "\n",
    "fig.legend(labels=[top2devices[0][12:],top2devices[1][12:]])\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1jbqJBqapP8"
   },
   "source": [
    "### iPhone or Android\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHtptHMCapP8"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "dist1 = dist1[dist1['time'].dt.year == 2016]\n",
    "dist2 = dist2[dist2['time'].dt.year == 2016]\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "sns.distplot(dist1['hour'], hist=False, kde=True)\n",
    "sns.distplot(dist2['hour'], hist=False, kde=True)\n",
    "\n",
    "fig.legend(labels=[top2devices[0][12:],top2devices[1][12:]])\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0zbpLZUapP9"
   },
   "source": [
    "### Device Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OEbR4afapP9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "\n",
    "df_trump['year'] = df_trump['time'].apply(year_fraction) #should be df_trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRdFIObiapP9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dist1 = df_trump[df_trump['source'] == top2devices[0]]\n",
    "dist2 = df_trump[df_trump['source'] == top2devices[1]]\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "sns.distplot(dist1['year'], kde=True)\n",
    "sns.distplot(dist2['year'], kde=True)\n",
    "fig.legend(labels=[top2devices[0][12:],top2devices[1][12:]])\n",
    "\n",
    "\n",
    "plt.xlabel(\"year\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn0opKxWapP9"
   },
   "source": [
    "##  Sentiment Analysis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NICSCaj_apP9",
    "outputId": "f2ddb09d-b75a-41c9-a12b-89eec667c346"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/akshaypozath/Library/Messages/Attachments/ea/10/6D0E76BA-2EA3-4C33-A35A-069EE9C29EFF'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(''.join(open(\"/home/ap1877/shared/vader_lexicon.txt\").readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqr5pItGapP9"
   },
   "source": [
    "### Polarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TR_upovHapP9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inde = []\n",
    "value = []\n",
    "for line in open(\"/home/ap1877/shared/vader_lexicon.txt\").readlines():\n",
    "    temp = re.split(r'\\t+', line)\n",
    "    inde.append(temp[0])\n",
    "    value.append(float(temp[1]))\n",
    "df_sent = pd.DataFrame(value, index=inde, columns=['polarity'])\n",
    "df_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP0VMBSFapP9"
   },
   "source": [
    "### Sentiment\n",
    "\n",
    "Use lexicon\n",
    "\n",
    "-For each tweet, find the sentiment of each word.\n",
    "-Sum of sentiments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5sBadfbapP9"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "df_trump['text'] = df_trump['text'].str.lower()\n",
    "df_trump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqK4HQNVapP-"
   },
   "source": [
    "### punctuations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNAvW1axapP-"
   },
   "outputs": [],
   "source": [
    "# Save your regex in punct_re\n",
    "punct_re = r'[^\\w\\s\\\\n]'\n",
    "\n",
    "regex = re.compile(punct_re)\n",
    "df_trump['no_punc'] = df_trump['text'].replace(regex,' ')\n",
    "df_trump.index = pd.to_numeric(df_trump.index)\n",
    "df_trump\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmaOn2D7apP-"
   },
   "outputs": [],
   "source": [
    "assert isinstance(punct_re, str)\n",
    "assert re.search(punct_re, 'this') is None\n",
    "assert re.search(punct_re, 'this is ok') is None\n",
    "assert re.search(punct_re, 'this is\\nok') is None\n",
    "assert re.search(punct_re, 'this is not ok.') is not None\n",
    "assert re.search(punct_re, 'this#is#ok') is not None\n",
    "assert re.search(punct_re, 'this^is ok') is not None\n",
    "assert df_trump['no_punc'].loc[800329364986626048] == 'i watched parts of nbcsnl saturday night live last night it is a totally one sided biased show nothing funny at all equal time for us'\n",
    "assert df_trump['text'].loc[884740553040175104] == 'working hard to get the olympics for the united states (l.a.). stay tuned!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KsT--c-apP-"
   },
   "source": [
    "### Tidy Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqaENYt3apP-"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "tidy_format = df_trump['no_punc'].str.split(expand=True)\n",
    "tidy_format = tidy_format.stack()\n",
    "tidy_format = tidy_format.reset_index()\n",
    "tidy_format.columns = ['ID', 'num', 'word']\n",
    "\n",
    "\n",
    "print(tidy_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPHMJKK1apP-"
   },
   "outputs": [],
   "source": [
    "assert tidy_format.loc[894661651760377856].shape == (27, 2)\n",
    "assert ' '.join(list(tidy_format.loc[894661651760377856]['word'])) == 'i think senator blumenthal should take a nice long vacation in vietnam where he lied about his service so he can at least say he was there'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCkJmueyapP-"
   },
   "source": [
    "### Polrity \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aem_gXlMapP-"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/akshaypozath/Library/Messages/Attachments/ea/10/6D0E76BA-2EA3-4C33-A35A-069EE9C29EFF'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "senti = tidy_format.merge(df_sent, how='left', left_on='word', right_index=True)\n",
    "\n",
    "senti = senti.groupby('ID').sum()\n",
    "df_trump['polarity'] = senti['polarity']\n",
    "\n",
    "\n",
    "\n",
    "df_trump.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhgNONHiapP-"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(df_trump.loc[744701872456536064, 'polarity'], 8.4)\n",
    "assert np.allclose(df_trump.loc[745304731346702336, 'polarity'], 2.5)\n",
    "assert np.allclose(df_trump.loc[744519497764184064, 'polarity'], 1.7)\n",
    "assert np.allclose(df_trump.loc[894661651760377856, 'polarity'], 0.2)\n",
    "assert np.allclose(df_trump.loc[894620077634592769, 'polarity'], 5.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iNmWEDNapP-"
   },
   "source": [
    "### Positive and Negative Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFanLgGlapP-"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEPERM: operation not permitted, scandir '/Users/akshaypozath/Library/Messages/Attachments/ea/10/6D0E76BA-2EA3-4C33-A35A-069EE9C29EFF'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('Most negative tweets:')\n",
    "\n",
    "\n",
    "neg = df_trump[['polarity', 'text']]\n",
    "neg = neg.sort_values('polarity', ascending=True)\n",
    "neg = neg.head(5)\n",
    "neg = neg['text']\n",
    "neg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b66zMSi6apP_"
   },
   "outputs": [],
   "source": [
    "print('Most positive tweets:')\n",
    "\n",
    "\n",
    "   \n",
    "pos = df_trump[['polarity', 'text']]\n",
    "pos = pos.sort_values('polarity', ascending=False)\n",
    "pos = pos.head(5)\n",
    "pos = pos['text']\n",
    "pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBTcV47lapP_"
   },
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ktQdImsapP_"
   },
   "outputs": [],
   "source": [
    "\n",
    "fox = df_trump[df_trump['text'].str.contains('fox')]\n",
    "nyt = df_trump[df_trump['text'].str.contains('nyt')]\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "sns.distplot(nyt['polarity'], kde=True)\n",
    "sns.distplot(fox['polarity'], kde=True)\n",
    "\n",
    "fig.legend(labels=['nyt','fox'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnpgkUcpapP_"
   },
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install nlkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "tmp = tidy_format.drop('num',axis=1)\n",
    "tmp = tmp.set_index(tmp.index)\n",
    "tmp\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.extend(['rt','t','co','https','realdonaldtrump','amp',\"u\",'hillary','trump2016','trump','clinton','http','ha','wa'])\n",
    "tmp = tmp[~tmp['word'].isin(stopwords)]\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "def lem(x):\n",
    "    return lemma.lemmatize(x)\n",
    "tmp['word'] = tmp.word.apply(lem)\n",
    "\n",
    "tmp = tmp[~(tmp['word'].str.isnumeric())]\n",
    "\n",
    "\n",
    "tmp = tmp[(tmp['word'].str.len() > 2)]\n",
    "\n",
    "tmp = tmp.reset_index()\n",
    "fifty = tmp['word'].value_counts(ascending=False).nlargest(50).to_frame()\n",
    "tmp2 = tmp[tmp['word'].isin(fifty.reset_index()['index'])]\n",
    "ids = tmp2['ID'].unique()\n",
    "ids = ids[:5000]\n",
    "matrix = np.zeros((5000,50))\n",
    "fifty = fifty.reset_index()\n",
    "\n",
    "for a in range(5000):\n",
    "    for b in range(50):\n",
    "        if (fifty['index'][b]) in df_trump['no_punc'].loc[ids[a]]:\n",
    "            matrix[a][b] += 1\n",
    "\n",
    "matrix[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRABWO6xapP_"
   },
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(matrix)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCjL4BbYapQA"
   },
   "source": [
    "### Examine the PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeF4AKiPapQA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "heatmap_df = pd.DataFrame(pca.components_)   \n",
    "heatmap_df['word'] = fifty['index']\n",
    "heatmap_df = heatmap_df.set_index('word')\n",
    "xlist = []\n",
    "for x in range(50):\n",
    "    xlist.append(\"PC\" + str(x+1))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "sns.heatmap(heatmap_df, xticklabels=xlist, yticklabels=True, cmap=\"YlGn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BASs-3ERapQA"
   },
   "source": [
    "### Compare PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw7mi75wapQA"
   },
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "graph = sns.JointGrid(data=heatmap_df, x = 1, y = 2)\n",
    "graph.plot(sns.scatterplot, sns.histplot)\n",
    "\n",
    "ax_joint = graph.ax_joint\n",
    "ax_marg_x = graph.ax_marg_x\n",
    "ax_marg_y = graph.ax_marg_y\n",
    "\n",
    "ax_joint.set_xticks([0.2,0, 0.2, 0.4, 0.6]) \n",
    "ax_joint.set_xlim(-0.2, 0.6)  \n",
    "\n",
    "\n",
    "ax_joint.set_yticks([-0.2, 0, 0.2, 0.4, 0.6,0.8])\n",
    "ax_joint.set_ylim(-0.3, 0.8)\n",
    "\n",
    "graph.set_axis_labels('PC1', 'PC2')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MidSemester Project - F21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
